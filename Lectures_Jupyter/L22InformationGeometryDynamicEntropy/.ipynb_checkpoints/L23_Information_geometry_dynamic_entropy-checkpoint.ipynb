{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<font size='7' style=\"color:#0D47A1\">  <b>INFORMATION GEOMETRY AND <br> <br>DYNAMIC ENTROPY</b></font>\n",
    "</center>\n",
    "\n",
    "<hr style= \"height:3px;\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6' style=\"color:#00A6D6\">  <b>Introduction. Fisher Information Metric</b></font>\n",
    "Adapted from \"Fisher information distance: a geometrical reading\" [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information geometry has provided analysis in domains such as statistical inference, information theory, mathematical programming and neurocomputing. It emerges as a result of the differential geometric structure on manifolds of probability distributions, with the Riemannian metric of the manifold being defined by the Fisher information matrix [1].\n",
    "\n",
    "In the case of information theory, the trace of the Fisher matrix is related to surface area of the set associated with a given probability distribution, whereas the volume of this set is related to the entropy. This fact was used to establish connections between information theory and differential geometry [1]. \n",
    "\n",
    "Current applications of information geometry in statistics include the problem of dimensionality reduction through information geometric methods on statistical manifolds. It has been shown that the dimension of a manifold of a probability density function is often intrinsically lower dimensional than the domain of the data space representation [2]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, many analytical techniques require some measure of dissimilarity between distributions of involved objects, or the substitution of a data set with its proper average. We investigate these measures focusing on statistcal models for normal probability distribution functions. Using the connection with classical hyperbolic geometry the Fisher distance is derived and connections to the Kullback-Leibler divergence are made. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geometric model of the (mean-standard deviation) half-plane associates each point in the\n",
    "half upper plane of $\\Re^2$ with a univariate Gaussian PDF \n",
    "\n",
    "$$ f(x, \\mu, \\sigma) = \\dfrac{1}{\\sqrt{2 \\pi \\sigma}}exp\\Big( \\dfrac{- \\ {|x-\\mu|}^2}{2\\sigma^2} \\Big) $$\n",
    "\n",
    "where $x$ is the single parameter, $\\sigma$ is the standard deviation, and $\\mu$ is the mean. \n",
    "\n",
    "The parameter space for this family of probability density funtions is given by \n",
    "\n",
    "$$ H = \\{ (\\mu, \\sigma) \\in \\Re^2 | \\ \\sigma > 0 \\}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distance between two points $P = (\\mu_1, \\sigma_1)$ and $Q= (\\mu_2, \\sigma_2)$ in the half-plane $H$ should reflect the dissimilarity between the associated probability density functions. \n",
    "\n",
    "![markov graph](/tree/Pictures/univariatenormaldistribution.png)\n",
    "\n",
    "![markov graph](/tree/Pictures/half-plane.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison between univariate normal distributions with fixed means and increasing standard deviation. The dissimilarity between the probabilities attached to the same interval concerning the distribution associated with C and D is smaller than the one between the distributions associated with A and B (top). This means that the distance between points C and D in the upper half-plane (bottom) should be closer to eachother than the points A and B, reflecting that the pair of distributions A and B is more dissimilar than the pair C and D. One can see this is not the case and so the distance between the two points cannot be a Euclidean metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proper distance arises from the Fisher information matrix, which is the measure of the amount of information in the location parameter. For univariate distributions parametrized by an $n$-dimensional space, the coefficients of this matrix define a metric. These coefficients are calculated as the expectation value of a product involving partial derivatives of the logarithm of the probability distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g_{ij}(\\beta) = \\int^{\\infty}_{\\infty}f(x,\\beta)  \\ \\dfrac{\\partial ln \\ f(x, \\beta)}{\\partial \\beta_i}\\dfrac{\\partial ln \\ f(x, \\beta)}{\\partial \\beta_j}dx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between two points $P,Q$ is given by the minimum length of all the smooth paths connecting the two points, in other words, a geodesic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distance expressed in terms of the Fisher metric is \n",
    "\n",
    "![markov graph](/tree/Pictures/fisher_half_plane.png)\n",
    "\n",
    "![markov graph](/tree/Pictures/fishermetric.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Stochastic Dynamics and Relative Entropy</b></font>\n",
    "\n",
    "An adaptation of chapter 12 from \"Functional Integration: Actions and Symmetries\" by Pierre Carter and Cecile DeWitt-Morette [4]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The telegraph equation is the simplest wave equation with dissipation: here $a$ and $v$ are positive constants and $\\Phi(x)$ is an arbitrary function \n",
    "\n",
    "$$ \\dfrac{1}{v^2}\\dfrac{\\partial^2F}{\\partial t^2}+\\dfrac{2a}{v^2}\\dfrac{\\partial F}{\\partial t}-\\dfrac{\\partial^2F}{\\partial x^2} = 0 $$\n",
    "\n",
    "$$ F(x,0)= \\Phi(x) $$\n",
    "\n",
    "$$ \\dfrac{\\partial}{\\partial t}F(x,t) \\rvert_{t=o} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stochastic solution is an average over the set of random walks defined by the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a lattice of points on $\\Re$ with lattice spacing $\\Delta x$ and a particle starting at time $t=0$ with a location of $x_0=x(0)$ and constant speed of $v$ in the positive or negative direction. \n",
    "\n",
    "Each step $\\Delta x$ is of duration $\\Delta t$;\n",
    "\n",
    "$$ \\Delta x = v \\Delta t. $$\n",
    "\n",
    "At each lattice point, \n",
    "\n",
    "the probability of reversing direction is $a\\Delta t ;$ \n",
    "\n",
    "the probability of maintaining direction is $1-a\\Delta t.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $vS_n$ be the displacement of the particle after $n$ steps, i.e., at time \n",
    "\n",
    "$$ t = n\\Delta t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the random variable $\\epsilon_i$ be defined such that\n",
    "\n",
    " \n",
    "$\\epsilon_i =\\begin{cases}\n",
    "    0& \\text{for no change of direction,}\\\\\n",
    "    1& \\text{for a change of direction}\n",
    "\\end{cases}$\n",
    "\n",
    "where $i=1,...,n.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random variables are stochastically independent and have the property \n",
    "\n",
    "$$ \\text{Pr}[\\epsilon_i = 0]=1-a\\Delta t $$\n",
    "$$ \\text{Pr}[\\epsilon_i = 1]=a\\Delta t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further quantify the number of changes of direction after $i$ steps through\n",
    "\n",
    "$$N_i = \\epsilon_1+\\cdots +\\epsilon_{i-1} $$\n",
    "\n",
    "for $i\\geq 1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention we let $N_1 = 0$ and write the displacement after $n$-steps as \n",
    "\n",
    "$$ vS_n = v\\Delta t\\sum_{i=1}^n (-1)^{N_i}$$\n",
    "\n",
    "where the quantity $S_n \\in [-t,t]$ is not necessarily positive. \n",
    "\n",
    "The solution $F_a(x_0,t)$ of the telegraph equation was given by M. Kak in 1956 as the average \n",
    "\n",
    "$$ F_a(x_0,t) = \\lim_{n=\\infty} \\dfrac{1}{2}\\langle \\Phi(x_0+vS_n)\\rangle + \\dfrac{1}{2}\\langle \\Phi(x_0-vS_n) \\rangle$$\n",
    "\n",
    "where the subscript $a$ represents a nonzero dissipation constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of zero dissipation, $a=0$, we obtain $S_n = t$, and the solution becomes a superposition of right-left moving pulses\n",
    "\n",
    "$$ F_0(x_0, t)= \\dfrac{1}{2}\\langle \\Phi(x_0+vt)\\rangle + \\dfrac{1}{2}\\langle \\Phi(x_0-vt) \\rangle $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can re-write the solution $F_a(x_0,t)$ in terms of $F_0(x_0,t)$ as the average \n",
    "\n",
    "$$ F_a(x_0,t) = \\lim_{n=\\infty} \\langle F_0(x_0,S_n) \\rangle $$\n",
    "\n",
    "where the symmmetry $F_0(x_0,t)=F_0(x_0,-t)$ is broken by the nonzero dissipation term $a$, forcing us to introduce the random variable $S_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kak then did away with discretization by introducing the random variable $S(t)$, the continuous counterpart of $S_n$, defined by\n",
    "\n",
    "$$ S(t) = \\int_0^t (-1)^{N(\\tau)}d\\tau \\quad S(t) \\in [-t,t]$$\n",
    "\n",
    "and its corresponding solution to the telegraph equation\n",
    "\n",
    "$$ F_a(x_0,t) = \\dfrac{1}{2}\\langle \\Phi(x_0+vS(t))\\rangle + \\dfrac{1}{2}\\langle \\Phi(x_0-vS(t)) \\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The introduction of the random variable $S(t)$, produces interesting consequences in the solution of the telegraph equation without dissipation, $a=0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a solution $\\Phi(x,t)$ of the wave equation without dissipation\n",
    "\n",
    "$$ \\dfrac{1}{v}\\dfrac{\\partial^2\\Phi}{\\partial t^2}-\\dfrac{\\partial^2\\Phi}{\\partial x^2}=0 $$\n",
    "\n",
    "where $\\Phi(x,0)$ is arbitrary and\n",
    "\n",
    "$$ \\dfrac{\\partial \\Phi}{\\partial t}\\rvert_{t=0} \\ =0 $$\n",
    "\n",
    "we define\n",
    "\n",
    "$$ F(x,t) = \\langle \\Phi(x,S(t)) \\rangle $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fact that $S(t) \\in [-t,t]$ we can exploit the symmetry $\\Phi(x,t)=\\Phi(x,-t)$ and re-write the right hand side of $F(x,t)$ as \n",
    "\n",
    "$$\\dfrac{1}{2}\\langle \\Phi(x,S(t)) \\rangle + \\dfrac{1}{2}\\langle \\Phi(x,-S(t)) \\rangle $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the continuous solution \n",
    "\n",
    "$$  F_a(x,t) = \\dfrac{1}{2}\\langle \\Phi(x_0+S(t))\\rangle + \\dfrac{1}{2}\\langle \\Phi(x_0-S(t)) \\rangle $$\n",
    "\n",
    "to the wave equation without dissipation that we saw earlier\n",
    "\n",
    "$$ \\dfrac{1}{v^2}\\dfrac{\\partial^2F}{\\partial t^2}+\\dfrac{2a}{v^2}\\dfrac{\\partial F}{\\partial t}-\\dfrac{\\partial^2F}{\\partial x^2} = 0 $$\n",
    "\n",
    "$$ F(x,0)= \\Phi(x,0) $$\n",
    "\n",
    "$$ \\dfrac{\\partial}{\\partial t}F(x,t) \\rvert_{t=o} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S(t)$ can be thought of as a path-dependent time reparametrization. Essentially, $|S(t)|$ is the time required for a particle moving with velocity $v$, with no reversal, to reach the point $x(t)=v|S(t)|.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can compute the probability density $g(t,r)$ of $S(t)$ and the probability density $h(t,r)$ of $|S(t)|$ through\n",
    "\n",
    "$$ h(t,r)dr = Pr[r<|S(t)|<r+dr] \\quad 0<r<t $$\n",
    "\n",
    "these probabilities are related by \n",
    "\n",
    "$$ h(t,r)=g(t,r)+g(t,-r) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this relationship we can express the solution of the telegraph equation with dissipation in terms of the solution $\\Phi$ of the wave equatiom without dissipation.\n",
    "\n",
    "$$ F(x,t)=\\int_{-t}^{t}dr \\  \\Phi(x,r)g(t,r) $$\n",
    "\n",
    "$$=\\int_0^t dr \\ \\Phi(x,r)h(t,r)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to notice that when $v=\\infty$ and $a=\\infty$ occur together, such that, $\\dfrac{2a}{v^2}$ remains constant, the limiting case of the telegraph equation becomes the diffusion equation\n",
    "\n",
    "$$ \\dfrac{1}{D}\\dfrac{\\partial F}{\\partial t} = \\dfrac{\\partial^2 F}{\\partial x^2} $$\n",
    "\n",
    "where $\\dfrac{2a}{v^2}=\\dfrac{1}{D}$ where $D$ is the diffusion constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some process that represents the transitions between two states $i$ and $j$ for $i\\neq j$ and $i, j \\in X$, it is natural to consider the probability of these allowed transitions, namely, the probability distribution $p_i$,\n",
    "$$ p_i = \\dfrac{P_i}{\\sum_jP_j}$$ where $P_i\\in X$ is the number of times the state $X_i$ is contained in set $X$, i.e., the population of $X_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these probabilities $p_i$ change with time? \n",
    "\n",
    "Here we follow a construction by John Baez [3];\n",
    "\n",
    "from the quotient rule we get:\n",
    "\n",
    "$$\\dfrac{dp_i}{dt} = \\dfrac{\\dfrac{d P_i}{d t}\\Big(\\sum_j P_j\\Big)-P_i\\Big(\\sum_j\\dfrac{d P_j}{d t}\\Big)}{\\Big(\\sum_j P_j\\Big)^2} $$\n",
    "\n",
    "In addition, if our states $X_i$ also depend on time then, we can assume the rate of change of the number of states in $X_i$, is proportional to $P_i$, with the \"constant of proportionality\", being any smooth function $F_i$ of the populations $P_i$ such that, $$ \\dfrac{dP_i}{dt}=F_i(P_1,...,P_n)P_i $$. \n",
    "\n",
    "We can now write $$ \\dfrac{dp_i}{dt} = \\dfrac{F_i(P)P_i\\Big(\\sum_j P_j\\Big)-P_i\\Big(\\sum_jF_j(P)P_j\\Big)}{\\Big(\\sum_j P_j\\Big)^2} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which simplifies to $$ \\dfrac{dp_i}{dt} = F_i(P)p_i-\\Big( \\sum_jF_j(P)p_j \\Big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closer look a the quantity $\\sum_jF_j(P)p_j$ reveals this to be the __average weight__ of, what we will now call the weighting function, $F_i(P)$. In other words, it is the expected, or mean, weight of a state $X_i$ being chosen at random from the total set of states $X$ \n",
    "\n",
    "$$\\langle F(P) \\rangle = \\sum_jF_j(P) \\ p_j.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we can write $$\\dfrac{dp_i}{dt}= \\Big( F_i(P)-\\langle F(P) \\rangle \\Big)p_i $$ \n",
    "\n",
    "which states that in order for the relative number of states in the $ith$ state to increase, their weight must exceed the average weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an expression describing the dynamics of the probability distribution $\\dot{p_i}$, we are in a position to introduce a second probability distribution $q_i$ and ask, given some measure of \"closeness\", how does the probability distribution $p_i(t)$ change relative to $q_i$? Does it get get \"closer\" to $q_i$, further, or not change at all, i.e., stay constant? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, when given two probability distributions $p$ and $q$ on a finte set $X$, __the information of $p$ relative to $q$__, is \n",
    "\n",
    "$$I(p,q)=\\sum_{i\\in X}p_i \\ ln\\Bigg(\\dfrac{p_i}{q_i}\\Bigg). $$\n",
    "\n",
    "This quantity is often known as the \"relative entropy\", \"information gain\", or the \"Kullback-Leibler divergence\".  As is traditional in scientific rhetoric, we will not be using any of the perfectly suitable aforementioned terms, and instead introduce our own called, __relative information__. This quantity provides a convenient measure of \"closeness\" between two probability distributions. One can see that when $p = q$, our relative information is zero, which is to say the two distributions are \"infinitely close\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we wish to calculate the relative information between the distributions $p_i$ and $q_i$ where only $p_i(t)$ depends on time and $q_i$ is fixed. This can be written as \n",
    "\n",
    "$$ \\dfrac{d}{dt}I(q,p(t)) = - \\sum _i\\dfrac{\\dot{p_i}}{p_i}q_i$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\dfrac{d}{dt}I(q,p(t)) = - \\sum _i \\Big( F_i(P)-\\langle F(P) \\rangle \\Big)q_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fact that the $q_i$ sum to one we can express it as the following \n",
    "$$ \\dfrac{d}{dt}I(q,p(t)) = \\langle F(P) \\rangle-\\sum _i  F_i(P)q_i $$\n",
    "\n",
    "$$ = \\sum_iF_i(P)(p_i-q_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and letting $F_i(P)$ be the components of a vector $F(P)$\n",
    "\n",
    "$$ \\dfrac{d}{dt}I(q,p(t)) = F(P(t))\\cdot(p(t)-q) $$ \n",
    "\n",
    "and it follows that the relative information $I(q,p(t))$ will be nonincreasing if and only if\n",
    "\n",
    "$$F(P(t))\\cdot(p(t)-q) \\leq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interpret the above inequality, and when does it hold? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer this question, we need to go deeper! Of course means being more general with our assumptions, so  we start by assuming our function $F$ is linear in the probabilities $p_j$, which allows us to write \n",
    "\n",
    "$$ F_i(P)= \\sum_{j=1}^n A_{ij}p_j $$\n",
    "\n",
    "for some matrix A, which we can call the __fitness matrix__. \n",
    "\n",
    "Taking a closer look, if each function $F_i(P)$ samples from the finite set $X$ of __pure states__, then the fitness matrix $A_{ij}$ determines the __potential gain in relative information__ between the pure state $i$ and the pure state $j$. A probability distribution of pure states is called a __mixed state__. \n",
    "\n",
    "We can write the vector of fitness functions as \n",
    "\n",
    "$$F(P_p) = Ap$$\n",
    "$$F(P_q) = Aq$$\n",
    "\n",
    "Thus, we can write the potential gain in relative information when $F_p(P)$ samples from the mixed state $p$ and $F_q(P)$ from the mixed state $q$ as,\n",
    "\n",
    "$$ p\\cdot A q.$$\n",
    "\n",
    "In the case where \n",
    "\n",
    "$$ q\\cdot Ap \\geq p\\cdot Ap $$\n",
    "\n",
    "we define the mixed state $q$ as a __dominant mixed state__ for all mixed states $p$. \n",
    "\n",
    "Therefore, if $p(t)$ evolves while $q$ is fixed, then the time derivative of the relative information becomes \n",
    "\n",
    "$$ \\dfrac{d}{dt}I(q,p(t))=(p(t)-q)\\cdot Ap(t) $$\n",
    "\n",
    "and adding the condition of $q$ being a dominant mixed state gives the result\n",
    "\n",
    "$$ \\dfrac{d}{dt}I(q,p(t))\\leq 0 $$\n",
    "\n",
    "which again shows that the relative information is nonincreasing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if the condition of $q$ being a dominant mixed state satisfies the above inequality, then our question becomes, what is the meaning of dominance in this context and why does it give a solution to our dynamical equations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can understand this as follows, consider the function $F(P)$ and two different probability distributions $p$ and $q$. The inner product $q\\cdot Ap$ will be the potential gain in relative information between the function $F_p(P)$, sampling from probability distribution $p$, and the function $F_{q}(P)$, sampling from the distribution $q$. Similarly, $p\\cdot Ap$ is the potential gain in relative information when two functions sample the same probability distribution $p$. In essence, the dominance of the mixed state $q$ states, the potential gain of relative information can never increase when using a probability distribution different than $q$. This implies that the relative information $\\dfrac{d}{dt}I(q,p(t))$ can never increase as $p(t)$ evolves, and leaves us to interpret $\\dfrac{d}{dt}I(q,p(t))$ as the amount of information \"left to learn\" as the distribution of the pure states $p$ approaches the dominant mixed state $q$. In other words, the dominant mixed state $q$ is a steady-state solution and does not depend on time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Markov Processes</b></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov process can be described starting with a directed graph whose nodes correpsond to states of some system, and whose edges correspond to transitions between these states. The transitions are weighted by some \"rate constants\", like this: \n",
    "\n",
    "![markov graph](/tree/Pictures/digraph1.gif)\n",
    "\n",
    "The rate constant of a transmisson from $i \\in X$ to $j \\in X$ represents the probabilty per time that an item hops from the $ith$ state to the $jth$ state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if $\\tau \\in T$ has source $i$ and target $j$ we write: $\\tau:i\\rightarrow j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a Markov process we can construct a function $H:X \\times X \\rightarrow \\Re$ that is represented by a square matrix called the __Hamiltonian__. If $i \\neq j$ we define \n",
    "\n",
    "$$H_{ij}=\\sum_{\\tau:j\\rightarrow i}r_{\\tau} $$ \n",
    "\n",
    "to be the sum of the rate constants of all the transitions from $j$ to $i$. The diagonal entries are given by \n",
    "\n",
    "$$H_{ii}= -\\sum_{\\tau:j\\rightarrow i; j\\neq i}r(\\tau)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when $i\\neq j$. Given this Markov process and a time-dependent probability distribution on $X$, we write \n",
    "\n",
    "\\begin{equation}\n",
    "\\dfrac{d}{dt}p(t) = Hp(t) \n",
    "\\end{equation}\n",
    "\n",
    "where $H$ is the Hamiltonian. In this context, $H_{ij}p_j$ is the rate at which population flows from state $j$ to state $i$, while the quantity $H_{ii}p_i$ is the flow of population out of state $i$. These diagonal values $H_{ii}$ are chosen such that total population is conserved, i.e., conservation of information. These conditions define $H$ to be __infinitesimal stochastic__, meaning that its off-diagonal terms are non-negative and the sum of the terms in each column sum to zero. \n",
    "\n",
    "$$H_{ij}\\geq 0 \\ if \\ i\\neq j \\ and \\ \\sum_i H_{ij}=0 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the general nature of the Markov process, its dynamical formalism, is often found among other equations describing the dynamics of randomly evolving systems, in particular, the __Kolmogorov equations__. The __Kolmogorov forward equation__ is given by \n",
    "\n",
    "$$ \\dfrac{d}{dt}G(t_2,t_1)=HG(t_2,t_1)$$\n",
    "\n",
    "where $G(t_2,t_2)$ is a square matrix that depends on two times $t_1$ and $t_2$ where $t_1,t_2 \\in \\Re$ and $t_1\\leq t_2$. An element $G_{ij}(t_2,t_1)$ is the probability that is the system in in the $jth$ state at time $t_1$, it will be in the $ith$ state at some later time $t_2$. \n",
    "\n",
    "When $t_2=t_1$ we demand that $G(t_2,t_1)$ be the identity matrix, and consequently, we can write the following \n",
    "\n",
    "$$ G(t_2,t_1) = e^{((t_2-t_1)H)}$$\n",
    "\n",
    "whenever $t_1\\leq t_2$. Taking the derivative of the above expression with respect to $t_1$ yields the __Kolmogorov backward equation__\n",
    "\n",
    "$$ \\dfrac{d}{dt_1}G(t_2,t_1)=-G(t_2,t_1)H. $$\n",
    "\n",
    "If $p(t_2)$ obeys $\\dfrac{d}{dt}p(t_2)=Hp(t_2)$, and $G(t_2,t_1)$ is a solution to the Kolmogorov forward equation,  then we have the following relationship between $p(t_1)$ and $p(t_2)$\n",
    "\n",
    "$$ p(t_2)=G(t_2,t_1)p(t_1) $$\n",
    "\n",
    "when $t_1\\leq t_2$. Thus, knowing $G(t_2,t_1)$ immediately gives us all solutions to the Hamiltonian differential. Extending this formalism to $X$ being an infinite set, as opposed to finite, is explained in the context of the \"__Fokker-Plank equation__\", which becomes relevant later in the discussion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the nice things features of working with a finite state space $X$ is that, every Markov process admits one or more __steady states__, i.e., probability distributions $q$ that obey \n",
    "\n",
    "$$ Hq=0 $$\n",
    "\n",
    "and are thus time-independent solutions of the corresponding differential equation. Fixing any one of the steady state solutions $q$, one can show\n",
    "\n",
    "$$ \\dfrac{d}{dt}I(q,p(t)) \\leq 0 $$\n",
    "\n",
    "which is the same in equality we derived by letting $q$ be a dominant state. However, in a Markov process we have an additional term\n",
    "\n",
    "$$ \\dfrac{d}{dt}I(q(t),p) \\leq 0  $$\n",
    "\n",
    "which one can show has an interesting interpretation in terms of statistical mechanics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistical mechanics we want to assign an __energy__ $E_i$ to each state such that the steady state probabilities $q_i$ are given by the __Boltzmann distribution__\n",
    "\n",
    "$$ q_i = \\dfrac{e^{-\\beta E_i}}{Z(\\beta)} $$\n",
    "\n",
    "where $\\beta$ is the inverse temperature $1/T$, and $Z(\\beta)$ is the normalizing function called the __partition function__ and given by\n",
    "\n",
    "$$ Z(\\beta) = \\sum_{i \\in X} e^{-\\beta E_i}$$\n",
    "\n",
    "to ensure the probabilities sum up to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the difference between two probability distributions $q_i$ and $q_j$ and solving for the differnece in energies yields\n",
    "\n",
    "$$ E_i-E_j= -\\dfrac{1}{\\beta} ln\\Big( \\dfrac{q_i}{q_j} \\Big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider a Markov process on a set $X$ with a steady state probability distribution $q$, whose energies and partition function satisfy the above expressions, then for any probability distribution $p$ on $X$ we can define the __expected energy__ as \n",
    "\n",
    "$$ \\langle E \\rangle_p = \\sum_{i \\in X} p_iE_i $$\n",
    "\n",
    "and the __entropy__\n",
    "\n",
    "$$ S(p)= -\\sum_{i\\in X}p_i ln(p_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above we can now define the __free energy__\n",
    "\n",
    "$$ F(p) = \\langle E \\rangle_p -TS(p) $$\n",
    "\n",
    "which is roughly speaking, the amount of \"available energy\" in the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the free energy as\n",
    "\n",
    "$$ F(p) = \\sum_{i\\in X}p_iE_i+Tp_iln(p_i) $$\n",
    "\n",
    "and using $E_i= -T (ln(q_i)+ln(Z))$ we obtain \n",
    "\n",
    "$$ F(p) = -T\\sum_{i\\in X}\\Big( p_iln(q_i)-p_iln(p_i)+p_iln(Z) \\Big) $$\n",
    "$$= T\\Big( I(p,q)-ln(Z)  \\Big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the special case where $p=q$, the relative information vanishes and we obtain \n",
    "\n",
    "$$ F(q)=-Tln(Z) $$\n",
    "\n",
    "and pugging into the expression above we have\n",
    "\n",
    "$$\\dfrac{F(p)-F(q)}{T} = I(p,q) $$\n",
    "\n",
    "which says that relative information is proportional to the difference in free energies between two probability distributions. \n",
    "\n",
    "Since relative information is nonnegative, we can deduce that any probability distribution $p$ has, at least, as much free energy as the steady state $q$.\n",
    "\n",
    "$$F(p)\\geq F(q) $$\n",
    "\n",
    "And if we decide to evolve $p(t)$ in time, the decrease of relative entropy given by \n",
    "\n",
    "$$ \\dfrac{d}{dt}I(p(t),q)\\leq 0$$\n",
    "\n",
    "implies that\n",
    "\n",
    "$$\\dfrac{d}{dt}F(p(t))\\leq 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is a direct relationship between the dynamics of relative information and free energy. At the heart of this is the decrease in relative entropy and how it is conncected to the approach of equilibrium as $t\\rightarrow \\infty$. In one case, equilibrium is given by a dominant mixed state, while in the Markov case, equilibrium is given by a steady state solution where all probability distributions approach this state as $t\\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>References</b></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www.ece.drexel.edu/walsh/Yunshu_InformationGeometry.pdf\n",
    "\n",
    "[2] https://pdfs.semanticscholar.org/8d94/e6be623a2fb81375bbf0f768695c5dd50279.pdf\n",
    "\n",
    "[3] http://math.ucr.edu/home/baez/information/information_geometry_1.html\n",
    "\n",
    "[4] http://indico.ictp.it/event/a04198/session/8/contribution/4/material/0/0.pdf\n",
    "\n",
    "[5] https://arxiv.org/pdf/1210.2354.pdf\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
