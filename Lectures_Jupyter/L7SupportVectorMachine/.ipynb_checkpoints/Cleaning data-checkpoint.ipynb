{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Today we are going to learn to clean data for SVM. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources: \n",
    "This notebook follows https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\n",
    "\n",
    "For Easy.py see http://www.developerstation.org/2011/03/simple-tutorial-on-using-libsvm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data to the format of an SVM package\n",
    "We will use sklearn.svm, it requires a list of data and a list of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "pathx='C:/Users/FSU/Documents/HW/data/task1_1_learn_X.csv'    \n",
    "pathy='C:/Users/FSU/Documents/HW/data/task1_1_learn_y.csv'\n",
    "pathx_2='C:/Users/FSU/Documents/HW/data/task1_1_test_X.csv'\n",
    "\n",
    "with open(pathx, newline='') as csvfile:\n",
    "    datax = csv.reader(csvfile, delimiter=' ', quoting = csv.QUOTE_NONNUMERIC)\n",
    "    dataX_train = list(datax)\n",
    "with open(pathx_2, newline='') as csvfile:\n",
    "    datax2 = csv.reader(csvfile, delimiter=' ', quoting = csv.QUOTE_NONNUMERIC)\n",
    "    dataX_test = list(datax2)\n",
    "with open(pathy, newline='') as csvfile:\n",
    "    datay = csv.reader(csvfile, delimiter=' ', quoting = csv.QUOTE_NONNUMERIC)\n",
    "    dataY_train =np.ravel(list(datay))\n",
    "\n",
    "print(\"Data is in the correct format for sklearn.svm\")\n",
    "print(\"Input format:list of vectors to train, vector of labels.\")\n",
    "print(\"_----------------------------------_\")\n",
    "print(\"Testing that the input is correct:\")\n",
    "test_clf =  SVC(kernel = 'rbf', class_weight = 'balanced')\n",
    "test_clf = test_clf.fit(dataX_train[:200], dataY_train[:200])\n",
    "test_y_predict = test_clf.predict(dataX_train[200:300])                       \n",
    "print(confusion_matrix(dataY_train[200:300], test_y_predict, labels = range(2)))\n",
    "print('accuracy: ',accuracy_score(dataY_train[200:300], test_y_predict))\n",
    "print('f1: ' ,f1_score(dataY_train[200:300], test_y_predict))\n",
    "print(\"Test classifier input passed.\")\n",
    "print(\"_----------------------------------_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we run the svc on raw data and it worked. Below we will remove outliers and normalize the data. It is important to apply the same normalization to the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's work with numpy arrays\n",
    "X_train = np.array(dataX_train)\n",
    "Y_train = np.array(dataY_train)\n",
    "X_test = np.array(dataX_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there outliers?\n",
    "We will use IsolationForest to find outliers. Ill explain more about Isolation Forest in a future jupyter note.\n",
    "### is the data weighted? \n",
    "We have two classes and we need to make sure the proportion among them is close to 1 (or that the number of representatives of one divided by the total amount is closed to .5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "# fit the model\n",
    "clf = IsolationForest(max_samples=250)\n",
    "clf = clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "\n",
    "X_no_Outliers = X_train[y_pred_train==1]\n",
    "Y_no_Outliers = Y_train[y_pred_train==1]\n",
    "print(\"Outliers removed\")\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(len(X_train), len(X_no_Outliers))\n",
    "print('percentage of points of class 1 {0}'.format(np.sum(Y_no_Outliers[Y_no_Outliers == 1])/len(Y_no_Outliers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $.47$ is close to $.5$ we dont need to weight the data.\n",
    "If needed, we can add a weight to SVM, for example svm.SVC(kernel='linear', class_weight={1: 10})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current data sets are \"X_no_Outliers\", \"Y_no_Outliers\" and \"X_test\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct simple scaling on the data\n",
    "Many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order.\n",
    "In \"preprocessing\" there is a function called StandardScaller that finds the coefficients to make our features centered and with standar deviation 1. We use it to convert our train data and our  test data with the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_no_Outliers)\n",
    "\n",
    "print(scaler.mean_)\n",
    "print(scaler.scale_)\n",
    "dataX = scaler.transform(X_no_Outliers)\n",
    "dataY = Y_no_Outliers\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"_______________________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current data sets are 'dataX', 'dataY' and 'X_test'. From 'dataX' we found the parameters and we used them to scale 'dataX' and 'X_test'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the RBF kernel $K(x,y)=e^{\\gamma||s-y||^2}$\n",
    "When the number of features is very large, one may just use the linear kernel.\n",
    "We split the train test to have a way to evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Use cross-validation to find the best parameter $C$, $\\gamma$\n",
    "Here we tell to the Grid search to use \"accuracy\" as the score to compare. You will need to modify this to test for the 4 different scores available in the first task.\n",
    "\n",
    "After we get the best parameters, we repeat the search with parameters closer to the previously found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "training_X, testing_X, training_Y = dataX, X_test, dataY\n",
    "# testing_Y) = train_test_split(dataX, dataY, train_size = .90, random_state = 1)\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score, greater_is_better=True)#This step is needed to personalize the score used by Grid search\n",
    "inner_cv = KFold(n_splits=4, shuffle=True)\n",
    "param_grid =[ {'C':[.00001, .001, 1, 1e2, 1e3, 1e4, 1e5], 'kernel':['linear']},\n",
    "{'C':[.00001, .01, 1, 1e2, 1e3, 1e4, 1e5],\n",
    "             'gamma':[.0001, .001, .01, .1, 1, 10, 100],'kernel':['rbf'] },\n",
    "]\n",
    "\n",
    "clf = GridSearchCV( SVC( class_weight = 'balanced'), param_grid, scoring = accuracy_scorer, cv = inner_cv)# lets you determine other scoring\n",
    "clf = clf.fit(training_X, training_Y)\n",
    "print(\"Classifier parameters found\")\n",
    "print(clf.best_params_)\n",
    "print(\"Now we make another search close to the previous parameters.\")\n",
    "\n",
    "c_v = clf.best_params_['C']\n",
    "g_v = clf.best_params_['gamma']\n",
    "param_grid =[ {\n",
    "'C':\n",
    "[c_v, c_v+e**(-9), c_v+e**(-5), c_v-e**(-5), c_v-e**(-9)],\n",
    "             'gamma':\n",
    "[g_v, g_v+e**(-7), g_v+e**(-10), g_v-e**(-7), g_v-e**(-10)],\n",
    "            'kernel':\n",
    "            ['rbf'] \n",
    "                  },]\n",
    "\n",
    "clf = GridSearchCV( SVC( class_weight = 'balanced'), param_grid, scoring = accuracy_scorer, cv = inner_cv)# lets you determine other scoring\n",
    "clf = clf.fit(training_X, training_Y)\n",
    "print(\"Classifier parameters found\")\n",
    "print(clf.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Grid scores on development set:\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.6f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the best parameter $C$ and $\\gamma$ to train the whole training set\n",
    "After the best (C, Î³) is found, the whole training set is trained again\n",
    "to generate the final classifier.\n",
    "The above approach works well for problems with thousands or more data points.\n",
    "For very large data sets a feasible approach is to randomly choose a subset of the\n",
    "data set, conduct grid-search on them, and then do a better-region-only grid-search\n",
    "on the complete data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(C= clf.best_params_['C'], gamma= clf.best_params_['gamma'], kernel = clf.best_params_['kernel'],  class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "Now we use it on the new data and save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"+++++++++++++++++++++++++++++++\")\n",
    "print(\"Evaluation on new data.\")\n",
    "presults = clf.predict(dataX2)\n",
    "name = 'experiment.csv' \n",
    "with open(name, 'w') as csvfile:\n",
    "    writing = csv.writer(csvfile, delimiter = ' ')\n",
    "    for line in presults:\n",
    "        writing.writerow(line)\n",
    "print(\"data saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is already a package that does all of this for you: easy.py, it is part of libsvm but the data has different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
